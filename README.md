# optimization_experiments

 - Gradient Descent method: greedy
 - Newton method: approximate the function by two-term Taylor expansion using Hessian and Jacobian
 - Gauss-Newton method: approximate the function by quadratic - then approximate the Hessian using the Jacobian of the (unsquared) objective function. This is specific to minizing sum-of-squares.
 - Levenberg-Marquardt method: interpolation between Gradient Descent and Gauss-Newton.
